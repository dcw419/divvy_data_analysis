import pandas as pd
import zipfile
import os
import glob
import time

import pandas as pd
import zipfile
import os
import glob
import time

# ==========================================
# ğŸš¨ æ ¸å¿ƒä¿®æ”¹ 1ï¼šå°†ç©ºé—´æ•°æ®ï¼ˆç»çº¬åº¦ï¼‰åŠ å…¥ç™½åå•
# ==========================================
STRATEGY_COLS = [
    'ride_id', 'rideable_type', 'started_at', 'ended_at', 
    'start_station_name', 'end_station_name', 'member_casual',
    'start_lat', 'start_lng'  # <--- æ–°å¢ï¼šè¿™å†³å®šäº†ä½ èƒ½ä¸èƒ½ç”»å‡ºç‰›é€¼çš„ GIS åœ°å›¾
]

def load_raw_data(data_dir):
    print(f"   [Loader] Scanning raw files in: {data_dir}")
    zip_files = glob.glob(os.path.join(data_dir, "*.zip"))
    zip_files.sort()
    
    if not zip_files:
        print("   âŒ No .zip files found!")
        return None

    df_list = []
    for f in zip_files:
        try:
            with zipfile.ZipFile(f, 'r') as z:
                csv_name = [n for n in z.namelist() if n.endswith('.csv') and not n.startswith('__')][0]
                with z.open(csv_name) as file:
                    temp_df = pd.read_csv(file, usecols=STRATEGY_COLS, parse_dates=['started_at', 'ended_at'])
                    df_list.append(temp_df)
                    print(f"   -> Loaded: {os.path.basename(f)} | Rows: {len(temp_df):,}")
        except Exception as e:
            print(f"   -> âš ï¸ Skipped {f}: {e}")

    if not df_list:
        return None
    return pd.concat(df_list, ignore_index=True)

def clean_data(df):
    print(f"   [Cleaner] Cleaning {len(df):,} rows...")
    df['duration_min'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60
    
    # ==========================================
    # ğŸš¨ æ ¸å¿ƒä¿®æ”¹ 2ï¼šè¿‡æ»¤ GPS ç¼ºå¤±çš„è„æ•°æ®
    # ==========================================
    mask = (df['duration_min'] >= 1) & (df['duration_min'] <= 1440) & \
           (df['start_station_name'].notna()) & (df['end_station_name'].notna()) & \
           (df['start_lat'].notna()) & (df['start_lng'].notna()) # <--- å¿…é¡»æœ‰ç»çº¬åº¦æ‰èƒ½ç”»å›¾
           
    df_clean = df.loc[mask].copy()
    
    # æ³¨æ„ï¼šstart_lat å’Œ start_lng å¿…é¡»ä¿æŒ Float æµ®ç‚¹æ•°ï¼Œä¸èƒ½è½¬ category
    for col in ['rideable_type', 'member_casual', 'start_station_name', 'end_station_name']:
        df_clean[col] = df_clean[col].astype('category')
        
    return df_clean

def get_processed_data(data_dir, cache_dir, force_reload=False):
    """
    æ™ºèƒ½æ•°æ®åŠ è½½å™¨ï¼šå°†ç¼“å­˜å­˜å…¥ cache_dir
    """
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
        
    cache_path = os.path.join(cache_dir, "cached_data.parquet")
    
    if os.path.exists(cache_path) and not force_reload:
        print(f"\n[âš¡ Cache Hit] Found cached data: {cache_path}")
        try:
            start_time = time.time()
            df = pd.read_parquet(cache_path)
            print(f"   âœ… Data Loaded in {time.time()-start_time:.2f}s! Rows: {len(df):,}")
            return df
        except Exception as e:
            print(f"   âš ï¸ Cache corrupted: {e}. Reloading raw data...")
    
    print(f"\n[ğŸ¢ Cache Miss] Loading from raw sources (This might take a while)...")
    raw_df = load_raw_data(data_dir)
    
    if raw_df is not None:
        clean_df = clean_data(raw_df)
        print(f"   ğŸ’¾ Saving cache to: {cache_path}")
        clean_df.to_parquet(cache_path, index=False)
        print("   âœ… Cache created successfully.")
        return clean_df
    return None
